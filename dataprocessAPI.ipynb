{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多帧图像转为视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# 图像文件夹路径\n",
    "image_folder = \"visualResult/CoDeFv1/beauty_1\"\n",
    "fps = 30.0\n",
    "\n",
    "# 视频输出路径和名称\n",
    "video_name = \"visualResult/CoDeFv1/beauty_1.avi\"\n",
    "\n",
    "# 获取图像文件夹中的所有图像文件名\n",
    "images = [img for img in os.listdir(image_folder)]\n",
    "\n",
    "# 读取第一张图像，获取图像尺寸\n",
    "frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "# 使用cv2.VideoWriter创建视频写入对象\n",
    "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (width, height))\n",
    "\n",
    "# 将每个图像逐帧写入视频\n",
    "for image in images:\n",
    "    video.write(cv2.imread(os.path.join(image_folder, image)))\n",
    "\n",
    "# 关闭视频写入对象\n",
    "video.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 采样图像尺寸为指定大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用PIL\n",
    "from PIL import Image\n",
    "\n",
    "def resize_image(input_image_path, output_image_path, size):\n",
    "    with Image.open(input_image_path) as image:\n",
    "        resized_image = image.resize(size)\n",
    "        resized_image.save(output_image_path)\n",
    "\n",
    "# 示例用法\n",
    "input_path = 'input.jpg'  # 输入图像路径\n",
    "output_path = 'output.jpg'  # 输出图像路径\n",
    "target_size = (800, 600)  # 目标大小，宽度为800像素，高度为600像素\n",
    "\n",
    "resize_image(input_path, output_path, target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用opencv\n",
    "# 指定新的图像尺寸\n",
    "import cv2\n",
    "\n",
    "# 加载图像\n",
    "image = cv2.imread('input_image.jpg')  # 替换为你的图像路径\n",
    "\n",
    "new_width = 500\n",
    "new_height = 300\n",
    "\n",
    "# 采样图像\n",
    "resized_image = cv2.resize(image, (new_width, new_height))\n",
    "# 保存采样后的图像\n",
    "cv2.imwrite('output_image.jpg', resized_image)  # 替换为你想保存的路径和文件名\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 光流warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_warp(x, flow, interp_mode='bilinear', padding_mode='zeros', align_corners=True):\n",
    "    \"\"\"Warp an image or feature map with optical flow.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): Tensor with size (n, c, h, w).\n",
    "        flow (Tensor): Tensor with size (n, h, w, 2), normal value.\n",
    "        interp_mode (str): 'nearest' or 'bilinear'. Default: 'bilinear'.\n",
    "        padding_mode (str): 'zeros' or 'border' or 'reflection'.\n",
    "            Default: 'zeros'.\n",
    "        align_corners (bool): Before pytorch 1.3, the default value is\n",
    "            align_corners=True. After pytorch 1.3, the default value is\n",
    "            align_corners=False. Here, we use the True as default.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Warped image or feature map.\n",
    "    \"\"\"\n",
    "    assert x.size()[-2:] == flow.size()[1:3]\n",
    "    _, _, h, w = x.size()\n",
    "    # create mesh grid\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(0, h).type_as(x), torch.arange(0, w).type_as(x))\n",
    "    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n",
    "    grid.requires_grad = False\n",
    "\n",
    "    vgrid = grid + flow\n",
    "    # scale grid to [-1,1]\n",
    "    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(w - 1, 1) - 1.0\n",
    "    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(h - 1, 1) - 1.0\n",
    "    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n",
    "    output = F.grid_sample(x, vgrid_scaled, mode=interp_mode, padding_mode=padding_mode, align_corners=align_corners)\n",
    "\n",
    "    # TODO, what if align_corners=False\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 裁剪任意尺寸的patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split(input_tensor, patch_size, overlap):\n",
    "    patch_h = patch_size[0]\n",
    "    patch_w = patch_size[1]\n",
    "    b,c,h,w = input_tensor.shape\n",
    "    h_space = np.arange(0, h-patch_h+1, patch_h-overlap)\n",
    "    if h - (h_space[-1] + patch_h) > 0:\n",
    "        h_space = np.append(h_space, h-patch_h)\n",
    "    w_space = np.arange(0, w-patch_w+1, patch_w-overlap)\n",
    "    if w - (w_space[-1] + patch_w) > 0:\n",
    "        w_space = np.append(w_space, w-patch_w)\n",
    "    \n",
    "    patches = []\n",
    "    for x in h_space:\n",
    "        tmp = []\n",
    "        for y in w_space:\n",
    "            tensor = input_tensor[:,:,x:x+patch_h,y:y+patch_w]\n",
    "            tmp.append(tensor)\n",
    "            # print(x,y, tensor.shape)\n",
    "        patches.append(tmp)\n",
    "    return patches, h_space, w_space\n",
    "\n",
    "def compose(input_tensor, patches, h_space, w_space, scale_factor ,is_latent=True):\n",
    "    \"\"\"\n",
    "    # h_space: array([   0,  492,  984, 1476, 1968, 2460, 2952, 3444, 3584])\n",
    "    # w_space: array([   0,  492,  984, 1476, 1968, 2460, 2560])\n",
    "    \"\"\"\n",
    "    b,c,h,w = input_tensor.shape\n",
    "    patch_h = patches[0][0].shape[-1]\n",
    "    patch_w = patches[0][0].shape[-2]\n",
    "    h_space = [int(h*scale_factor) for h in h_space]\n",
    "    w_space = [int(w*scale_factor) for w in w_space]\n",
    "    h = int(h * scale_factor)\n",
    "    w = int(w * scale_factor)\n",
    "    if is_latent:\n",
    "        h_space = [h//8 for h in h_space]\n",
    "        w_space = [w//8 for w in w_space]\n",
    "        h = h // 8\n",
    "        w = w // 8\n",
    "        c = patches[0][0].shape[1]\n",
    "    \n",
    "    \n",
    "    # 组合\n",
    "    output_tensor = torch.zeros(b, c, h, w).cpu()\n",
    "    for i,x in enumerate(h_space):\n",
    "        for j,y in enumerate(w_space):\n",
    "            output_tensor[:,:,x:x+patch_h, y:y+patch_w] += patches[i][j].cpu()\n",
    "    # 对重叠区域进行平均处理\n",
    "    pred_x_end = 0\n",
    "    for x in h_space:\n",
    "        if x == 0:\n",
    "            pred_x_end = x + patch_h\n",
    "            continue\n",
    "        output_tensor[:,:,x:pred_x_end,:] /= 2\n",
    "        pred_x_end = x + patch_h\n",
    "    pred_y_end = 0\n",
    "    for y in w_space:\n",
    "        if y == 0:\n",
    "            pred_y_end = y + patch_w\n",
    "            continue\n",
    "        output_tensor[:,:,:,y:pred_y_end] /= 2\n",
    "        pred_y_end = y + patch_w\n",
    "    return output_tensor\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "def read_img(img_path):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        b c h w\n",
    "    \"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB).astype(\"float32\")\n",
    "    img = torch.from_numpy(img.transpose(2,0,1)).float()/255.0\n",
    "    return img.unsqueeze(0)\n",
    "\n",
    "import torchvision.transforms.functional as tf\n",
    "def tensor2img(tensor,name):\n",
    "    \"\"\"\n",
    "    tensor:\n",
    "        c h w\n",
    "    \"\"\"\n",
    "    image = tf.to_pil_image(tensor)\n",
    "    image.save(name)\n",
    "\n",
    "import os\n",
    "\n",
    "def getSplitBox(img, patch_size, overlap):\n",
    "    b,c,h,w = img.shape\n",
    "    h_space = np.arange(0, h-patch_size+1, patch_size-overlap)\n",
    "    if h - (h_space[-1] + patch_size) > 0:\n",
    "        h_space = np.append(h_space, h-patch_size)\n",
    "    w_space = np.arange(0, w-patch_size+1, patch_size-overlap)\n",
    "    if w - (w_space[-1] + patch_size) > 0:\n",
    "        w_space = np.append(w_space, w-patch_size)\n",
    "    return h_space, w_space\n",
    "\n",
    "def composeFromFiles(patch_path, patch_size, overlap):\n",
    "    patch_list = os.listdir(patch_path)\n",
    "    img = torch.zeros(1,3, 3072, 4096)\n",
    "    h_space, w_space = getSplitBox(img, patch_size, overlap)\n",
    "    n_h = len(h_space)\n",
    "    n_w = len(w_space)\n",
    "    patches = [[0]*n_w for _ in range(n_h)]\n",
    "    print(len(patch_list))\n",
    "    for p in patch_list:\n",
    "        print(p)\n",
    "        p_name,ext = p.split(\".\")\n",
    "        if p_name == \"compose\":\n",
    "            continue\n",
    "        h,w = p_name.split(\"_\")\n",
    "        print(h, w)\n",
    "        h = int(h)\n",
    "        w = int(w)\n",
    "        patches[h][w] = read_img(os.path.join(patch_path, p))\n",
    "    compose_img = compose(img, patches, h_space, w_space, patch_size ,1, False)\n",
    "    tensor2img(compose_img[0], os.path.join(patch_path, f\"patch_compose.png\"))\n",
    "\n",
    "# input_tensor = torch.randn(1,3,4096,3075)\n",
    "input_tensor = read_img(\"/data2/lvjiangtao/vivo/ModelZoo/SUPIR/ljtdir/input_image/SUPIR_maskGaussianGuidedFilter.png\")\n",
    "patch_size= 256\n",
    "overlap = 20\n",
    "scale_factor = 1024 // patch_size\n",
    "patches,h_space,w_space = split(input_tensor, patch_size, overlap)\n",
    "# h_space: array([   0,  492,  984, 1476, 1968, 2460, 2952, 3444, 3584])\n",
    "# w_space: array([   0,  492,  984, 1476, 1968, 2460, 2560])\n",
    "new_patches = []\n",
    "for tmp in patches:\n",
    "    new_tmp = []\n",
    "    for patch in tmp:\n",
    "        patch = torch.nn.functional.interpolate(patch, scale_factor=scale_factor)\n",
    "        patch = torch.nn.functional.interpolate(patch, scale_factor=1/8)\n",
    "        new_tmp.append(patch)\n",
    "    new_patches.append(new_tmp)\n",
    "print(\"patch shape:\", new_patches[0][0].shape)\n",
    "output_tensor = compose(input_tensor, new_patches, h_space.copy(), w_space.copy(), patch_size, scale_factor)\n",
    "print(\"output tensor:\", output_tensor.shape)\n",
    "output_tensor = torch.nn.functional.interpolate(output_tensor, scale_factor=1/scale_factor)\n",
    "output_tensor = torch.nn.functional.interpolate(output_tensor, scale_factor=8)\n",
    "tensor2img(output_tensor[0], \"/data2/lvjiangtao/vivo/ModelZoo/SUPIR/ljtdir/output_img/compose.png\")\n",
    "# print(output_tensor.shape)\n",
    "# print(torch.allclose(input_tensor, output_tensor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

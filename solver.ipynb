{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小波变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dwt_init(x):\n",
    "    x01 = x[:, :, 0::2, :] / 2\n",
    "    x02 = x[:, :, 1::2, :] / 2\n",
    "    x1 = x01[:, :, :, 0::2]\n",
    "    x2 = x02[:, :, :, 0::2]\n",
    "    x3 = x01[:, :, :, 1::2]\n",
    "    x4 = x02[:, :, :, 1::2]\n",
    "    x_LL = x1 + x2 + x3 + x4\n",
    "    x_HL = -x1 - x2 + x3 + x4\n",
    "    x_LH = -x1 + x2 - x3 + x4\n",
    "    x_HH = x1 - x2 - x3 + x4\n",
    "    return torch.cat((x_LL, x_HL, x_LH, x_HH), 0)\n",
    "\n",
    "\n",
    "# 使用哈尔 haar 小波变换来实现二维离散小波\n",
    "def iwt_init(x):\n",
    "    r = 2\n",
    "    in_batch, in_channel, in_height, in_width = x.size()\n",
    "    #print([in_batch, in_channel, in_height, in_width])\n",
    "    out_batch, out_channel, out_height, out_width = int(in_batch / r ** 2), int(in_channel), r * in_height, r * in_width\n",
    "    x1 = x[0:out_batch, :, :, :] / 2\n",
    "    x2 = x[out_batch:out_batch * 2, :, :, :] / 2\n",
    "    x3 = x[out_batch * 2:out_batch * 3, :, :, :] / 2\n",
    "    x4 = x[out_batch * 3:out_batch * 4, :, :, :] / 2\n",
    "\n",
    "    h = torch.zeros([out_batch, out_channel, out_height, out_width]).float().cuda()\n",
    "    h[:, :, 0::2, 0::2] = x1 - x2 - x3 + x4\n",
    "    h[:, :, 1::2, 0::2] = x1 - x2 + x3 - x4\n",
    "    h[:, :, 0::2, 1::2] = x1 + x2 - x3 - x4\n",
    "    h[:, :, 1::2, 1::2] = x1 + x2 + x3 + x4\n",
    "\n",
    "    return h\n",
    "\n",
    "# 二维离散小波\n",
    "class DWT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DWT, self).__init__()\n",
    "        self.requires_grad = False  # 信号处理，非卷积运算，不需要进行梯度求导\n",
    "\n",
    "    def forward(self, x):\n",
    "        return dwt_init(x)\n",
    "\n",
    "\n",
    "# 逆向二维离散小波\n",
    "class IWT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IWT, self).__init__()\n",
    "        self.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return iwt_init(x)\n",
    "\n",
    "\n",
    "class HaarDownsampling(nn.Module):\n",
    "    def __init__(self, channel_in):\n",
    "        super(HaarDownsampling, self).__init__()\n",
    "        self.channel_in = channel_in\n",
    "\n",
    "        self.haar_weights = torch.ones(4, 1, 2, 2)\n",
    "\n",
    "        self.haar_weights[1, 0, 0, 1] = -1\n",
    "        self.haar_weights[1, 0, 1, 1] = -1\n",
    "\n",
    "        self.haar_weights[2, 0, 1, 0] = -1\n",
    "        self.haar_weights[2, 0, 1, 1] = -1\n",
    "\n",
    "        self.haar_weights[3, 0, 1, 0] = -1\n",
    "        self.haar_weights[3, 0, 0, 1] = -1\n",
    "\n",
    "        self.haar_weights = torch.cat([self.haar_weights] * self.channel_in, 0)\n",
    "        self.haar_weights = nn.Parameter(self.haar_weights)\n",
    "        self.haar_weights.requires_grad = False\n",
    "\n",
    "    def forward(self, x, rev=False):\n",
    "        if not rev:\n",
    "            out = F.conv2d(x, self.haar_weights, bias=None, stride=2, groups=self.channel_in) / 4.0\n",
    "            out = out.reshape([x.shape[0], self.channel_in, 4, x.shape[2] // 2, x.shape[3] // 2])\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "            out = out.reshape([x.shape[0], self.channel_in * 4, x.shape[2] // 2, x.shape[3] // 2])\n",
    "            return out[:,:self.channel_in,:,:],out[:,self.channel_in:self.channel_in*4,:,:]\n",
    "        else:\n",
    "            out = x.reshape([x.shape[0], 4, self.channel_in, x.shape[2], x.shape[3]])\n",
    "            out = torch.transpose(out, 1, 2)\n",
    "            out = out.reshape([x.shape[0], self.channel_in * 4, x.shape[2], x.shape[3]])\n",
    "            return F.conv_transpose2d(out, self.haar_weights, bias=None, stride=2, groups = self.channel_in)\n",
    "\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels, out_channels):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n",
    "        self.conv3 = nn.Conv2d(32, out_channels, kernel_size=5, padding=5 // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.DWT = DWT()\n",
    "        self.IDWT = IWT()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.DWT(x)\n",
    "        print(x.shape)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.IDWT(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
